{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "559de805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "\n",
    "FEATURE_LAYER = 16  # Layer in VGG16 to extract features\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Similarity threshold for detection\n",
    "TEMPLATE_SIZE = (32, 32)  # Size of template for matching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66318378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "import os\n",
    "# --- NEW: helper to auto-crop a template (remove blank margins) ---\n",
    "def auto_crop_digit(gray_tpl, pad=1):\n",
    "    _, bin_inv = cv2.threshold(gray_tpl, 0, 255,\n",
    "                               cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    ys, xs = np.where(bin_inv > 0)\n",
    "    if len(xs) == 0:\n",
    "        return gray_tpl  # fallback\n",
    "    y1, y2 = max(0, ys.min()-pad), min(gray_tpl.shape[0], ys.max()+1+pad)\n",
    "    x1, x2 = max(0, xs.min()-pad), min(gray_tpl.shape[1], xs.max()+1+pad)\n",
    "    return gray_tpl[y1:y2, x1:x2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ac55a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_noise_removal(gray_img):\n",
    "    \"\"\"Remove lines and dots while preserving digits\"\"\"\n",
    "    \n",
    "    # 1. Remove small connected components (dots)\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(\n",
    "        cv2.threshold(gray_img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "    )\n",
    "    \n",
    "    # Filter out small components (likely dots)\n",
    "    min_area = 20  # Adjust based on your image size\n",
    "    cleaned = np.zeros_like(gray_img)\n",
    "    \n",
    "    for i in range(1, num_labels):\n",
    "        if stats[i, cv2.CC_STAT_AREA] >= min_area:\n",
    "            cleaned[labels == i] = 255\n",
    "    \n",
    "    # 2. Remove thin lines using morphological operations\n",
    "    # Vertical line removal\n",
    "    vertical_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 10))\n",
    "    vertical_lines = cv2.morphologyEx(cleaned, cv2.MORPH_OPEN, vertical_kernel)\n",
    "    cleaned = cv2.subtract(cleaned, vertical_lines)\n",
    "    \n",
    "    # Horizontal line removal\n",
    "    horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (10, 1))\n",
    "    horizontal_lines = cv2.morphologyEx(cleaned, cv2.MORPH_OPEN, horizontal_kernel)\n",
    "    cleaned = cv2.subtract(cleaned, horizontal_lines)\n",
    "    \n",
    "    # Convert back to grayscale\n",
    "    cleaned = cv2.bitwise_not(cleaned)\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "def robust_preprocessing(img_path, noise_reduction=True, contrast_enhance=True):\n",
    "    \"\"\"Enhanced preprocessing for noisy/warped captchas\"\"\"\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    if noise_reduction:\n",
    "        # Remove lines and dots\n",
    "        img = advanced_noise_removal(img)\n",
    "        \n",
    "        # Bilateral filter to reduce remaining noise while preserving edges\n",
    "        img = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "        \n",
    "        # Morphological opening to clean up\n",
    "        kernel = np.ones((2,2), np.uint8)\n",
    "        img = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)\n",
    "    \n",
    "    if contrast_enhance:\n",
    "        # CLAHE for adaptive contrast enhancement\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "        img = clahe.apply(img)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "039aa263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 1. Load CNN (feature extractor)\n",
    "# --------------------------\n",
    "cnn = models.vgg16(pretrained=True).features[:FEATURE_LAYER].to(DEVICE).eval()\n",
    "\n",
    "# Image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "def extract_feat_map(pil_img):\n",
    "    t = transform(pil_img).unsqueeze(0).to(DEVICE)      # [1,3,H,W]\n",
    "    with torch.no_grad():\n",
    "        fm = cnn(t)                                # [1,C,Hf,Wf], strideâ‰ˆ8\n",
    "    return fm.squeeze(0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "fcd28b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(img):\n",
    "    img_tensor = transform(img).unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        feat = cnn(img_tensor)\n",
    "    return feat.squeeze(0)  # Shape: [C, H, W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "eb727ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit 0: originals=5 final_templates=80\n",
      "Digit 1: originals=5 final_templates=80\n",
      "Digit 2: originals=5 final_templates=80\n",
      "Digit 3: originals=5 final_templates=80\n",
      "Digit 4: originals=5 final_templates=80\n",
      "Digit 5: originals=5 final_templates=80\n",
      "Digit 6: originals=5 final_templates=80\n",
      "Digit 7: originals=5 final_templates=80\n",
      "Digit 8: originals=5 final_templates=66\n",
      "Digit 9: originals=5 final_templates=80\n",
      "Precomputing CNN features for templates...\n",
      "Digit 0: 80 templates processed\n",
      "Digit 1: 80 templates processed\n",
      "Digit 2: 80 templates processed\n",
      "Digit 3: 80 templates processed\n",
      "Digit 4: 80 templates processed\n",
      "Digit 5: 80 templates processed\n",
      "Digit 6: 80 templates processed\n",
      "Digit 7: 80 templates processed\n",
      "Digit 8: 66 templates processed\n",
      "Digit 9: 80 templates processed\n"
     ]
    }
   ],
   "source": [
    "BASE_TARGET_HEIGHTS = [16, 20, 24,32]          # Much smaller!\n",
    "SCALE_FACTORS = [0.8, 0.9, 1.0, 1.1]        # enlarge / shrink templates\n",
    "DEDUP_MSE_THRESH = 15.0\n",
    "\n",
    "def build_box_templates(template_dir=\"templates\",\n",
    "                        base_heights=BASE_TARGET_HEIGHTS,\n",
    "                        scale_factors=SCALE_FACTORS,\n",
    "                        dedup=True,\n",
    "                        mse_thresh=DEDUP_MSE_THRESH):\n",
    "    out = {}\n",
    "    for d in range(10):\n",
    "        paths = glob.glob(os.path.join(template_dir, str(d), \"*.*\"))\n",
    "        lst = []\n",
    "        for p in paths:\n",
    "            if not p.lower().endswith((\".png\",\".jpg\",\".jpeg\")): \n",
    "                continue\n",
    "            base = cv2.imread(p, cv2.IMREAD_GRAYSCALE)\n",
    "            if base is None:\n",
    "                continue\n",
    "            base = cv2.normalize(base, None, 0, 255, cv2.NORM_MINMAX)\n",
    "            base = auto_crop_digit(base)  # <-- NEW: tighter source template\n",
    "            for bh in base_heights:\n",
    "                scale0 = bh / max(1, base.shape[0])\n",
    "                w0 = max(8, int(base.shape[1] * scale0))\n",
    "                base_scaled = cv2.resize(base, (w0, bh), interpolation=cv2.INTER_LINEAR)\n",
    "                for sf in scale_factors:\n",
    "                    h2 = max(8, int(bh * sf))\n",
    "                    w2 = max(8, int(w0 * sf))\n",
    "                    tpl2 = cv2.resize(base_scaled, (w2, h2), interpolation=cv2.INTER_LINEAR)\n",
    "                    lst.append({'img': tpl2, 'h': h2, 'w': w2})\n",
    "        # Deduplicate by MSE (same size only)\n",
    "        if dedup:\n",
    "            kept = []\n",
    "            for t in lst:\n",
    "                add = True\n",
    "                for k in kept:\n",
    "                    if k['h'] == t['h'] and k['w'] == t['w']:\n",
    "                        mse = np.mean((k['img'].astype(np.float32) - t['img'].astype(np.float32))**2)\n",
    "                        if mse < mse_thresh:\n",
    "                            add = False\n",
    "                            break\n",
    "                if add:\n",
    "                    kept.append(t)\n",
    "            lst = kept\n",
    "        out[str(d)] = lst\n",
    "        print(f\"Digit {d}: originals={len(paths)} final_templates={len(lst)}\")\n",
    "    return out\n",
    "def precompute_cnn_features(pixel_templates):\n",
    "    \"\"\"Precompute CNN features for templates\"\"\"\n",
    "    print(\"Precomputing CNN features for templates...\")\n",
    "    template_features = {}\n",
    "    \n",
    "    for digit, templates in pixel_templates.items():\n",
    "        template_features[digit] = []\n",
    "        \n",
    "        for i, template_info in enumerate(templates):\n",
    "            template_gray = template_info['img']\n",
    "            h, w = template_gray.shape\n",
    "            \n",
    "            # Convert to RGB for CNN\n",
    "            template_rgb = cv2.cvtColor(template_gray, cv2.COLOR_GRAY2RGB)\n",
    "            template_pil = Image.fromarray(template_rgb)\n",
    "            \n",
    "            try:\n",
    "                features = extract_features(template_pil)\n",
    "                template_features[digit].append({\n",
    "                    'features': features,\n",
    "                    'height': h,\n",
    "                    'width': w\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing template {digit}_{i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Digit {digit}: {len(template_features[digit])} templates processed\")\n",
    "    \n",
    "    return template_features\n",
    "box_templates = build_box_templates()\n",
    "template_features = precompute_cnn_features(box_templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d2de955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_touching_digits(boxes, gray_img, min_width=8):\n",
    "    \"\"\"Attempt to separate touching digits using vertical projections\"\"\"\n",
    "    separated_boxes = []\n",
    "    \n",
    "    for x1, y1, x2, y2 in boxes:\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "        \n",
    "        # If box is suspiciously wide, try to split it\n",
    "        if width > height * 1.8:  # Likely contains multiple digits\n",
    "            digit_region = gray_img[y1:y2, x1:x2]\n",
    "            \n",
    "            # Invert and get vertical projection\n",
    "            _, binary = cv2.threshold(digit_region, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "            vertical_proj = np.sum(binary, axis=0)\n",
    "            \n",
    "            # Find valleys (potential split points)\n",
    "            valleys = []\n",
    "            for i in range(1, len(vertical_proj) - 1):\n",
    "                if (vertical_proj[i] < vertical_proj[i-1] and \n",
    "                    vertical_proj[i] < vertical_proj[i+1] and\n",
    "                    vertical_proj[i] < np.mean(vertical_proj) * 0.3):\n",
    "                    valleys.append(i)\n",
    "            \n",
    "            if valleys:\n",
    "                # Split at the middle valley\n",
    "                split_x = valleys[len(valleys)//2] + x1\n",
    "                \n",
    "                # Create two boxes\n",
    "                if split_x - x1 >= min_width and x2 - split_x >= min_width:\n",
    "                    separated_boxes.append([x1, y1, split_x, y2])\n",
    "                    separated_boxes.append([split_x, y1, x2, y2])\n",
    "                    continue\n",
    "        \n",
    "        separated_boxes.append([x1, y1, x2, y2])\n",
    "    \n",
    "    return separated_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "17a9a65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_feature_matching(img_path, templates_features, similarity_thresh=0.85):\n",
    "    \"\"\"Use CNN features for more robust matching\"\"\"\n",
    "    # Load and preprocess image\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_features = extract_features(img)  # Shape: [C, H, W]\n",
    "    \n",
    "    detections = []\n",
    "    \n",
    "    # Sliding window approach on feature map\n",
    "    for digit, template_list in templates_features.items():\n",
    "        for template_feat in template_list:\n",
    "            # Compute normalized cross-correlation in feature space\n",
    "            feat_h, feat_w = img_features.shape[1], img_features.shape[2]\n",
    "            tpl_h, tpl_w = template_feat['features'].shape[1], template_feat['features'].shape[2]\n",
    "            \n",
    "            if feat_h < tpl_h or feat_w < tpl_w:\n",
    "                continue\n",
    "                \n",
    "            # Slide template over image features\n",
    "            for y in range(feat_h - tpl_h + 1):\n",
    "                for x in range(feat_w - tpl_w + 1):\n",
    "                    img_patch = img_features[:, y:y+tpl_h, x:x+tpl_w]\n",
    "                    \n",
    "                    # Cosine similarity\n",
    "                    similarity = F.cosine_similarity(\n",
    "                        img_patch.flatten().unsqueeze(0),\n",
    "                        template_feat['features'].flatten().unsqueeze(0)\n",
    "                    ).item()\n",
    "                    \n",
    "                    if similarity > similarity_thresh:\n",
    "                        # Map back to pixel coordinates (approximate stride=8)\n",
    "                        stride = 8\n",
    "                        px1, py1 = x * stride, y * stride\n",
    "                        px2 = px1 + template_feat['width']\n",
    "                        py2 = py1 + template_feat['height']\n",
    "                        \n",
    "                        detections.append([px1, py1, px2, py2, similarity])\n",
    "    \n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b25439af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "def refine_box(gray, x1, y1, x2, y2, min_size=3, dilate_iters=1, margin=1):\n",
    "    \"\"\"\n",
    "    Tighten a raw template match box to foreground strokes.\n",
    "    dilate_iters: small dilation to avoid cutting thin parts\n",
    "    margin: expand final tight box a little so it is not over-tight\n",
    "    \"\"\"\n",
    "    h, w = gray.shape\n",
    "    x1 = max(0, x1); y1 = max(0, y1)\n",
    "    x2 = min(w-1, x2); y2 = min(h-1, y2)\n",
    "    if x2 <= x1+1 or y2 <= y1+1:\n",
    "        return x1,y1,x2,y2\n",
    "    patch = gray[y1:y2, x1:x2]\n",
    "    # Otsu inverse (assuming dark digit)\n",
    "    _, bin_inv = cv2.threshold(patch, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    if dilate_iters > 0:\n",
    "        k = np.ones((3,3), np.uint8)\n",
    "        bin_inv = cv2.dilate(bin_inv, k, iterations=dilate_iters)\n",
    "    ys, xs = np.where(bin_inv > 0)\n",
    "    if len(xs) == 0:\n",
    "        return x1,y1,x2,y2\n",
    "    ny1, ny2 = y1 + ys.min(), y1 + ys.max() + 1\n",
    "    nx1, nx2 = x1 + xs.min(), x1 + xs.max() + 1\n",
    "    if (nx2-nx1) < min_size or (ny2-ny1) < min_size:\n",
    "        return x1,y1,x2,y2\n",
    "    # Add small margin (clamped)\n",
    "    nx1 = max(0, nx1 - margin)\n",
    "    ny1 = max(0, ny1 - margin)\n",
    "    nx2 = min(w-1, nx2 + margin)\n",
    "    ny2 = min(h-1, ny2 + margin)\n",
    "    return nx1, ny1, nx2, ny2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bea50440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms_xyxy(boxes_scores, iou_threshold=0.3):\n",
    "    \"\"\"\n",
    "    boxes_scores: list [x1,y1,x2,y2,score]\n",
    "    \"\"\"\n",
    "    if not boxes_scores: \n",
    "        return []\n",
    "    arr = np.array(boxes_scores, dtype=float)\n",
    "    x1,y1,x2,y2,sc = arr[:,0],arr[:,1],arr[:,2],arr[:,3],arr[:,4]\n",
    "    order = sc.argsort()[::-1]\n",
    "    keep = []\n",
    "    while order.size > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "        if order.size == 1:\n",
    "            break\n",
    "        rest = order[1:]\n",
    "        xx1 = np.maximum(x1[i], x1[rest])\n",
    "        yy1 = np.maximum(y1[i], y1[rest])\n",
    "        xx2 = np.minimum(x2[i], x2[rest])\n",
    "        yy2 = np.minimum(y2[i], y2[rest])\n",
    "        iw = np.clip(xx2 - xx1, 0, None)\n",
    "        ih = np.clip(yy2 - yy1, 0, None)\n",
    "        inter = iw * ih\n",
    "        area_i = (x2[i]-x1[i])*(y2[i]-y1[i])\n",
    "        area_r = (x2[rest]-x1[rest])*(y2[rest]-y1[rest])\n",
    "        union = area_i + area_r - inter\n",
    "        iou = inter / (union + 1e-6)\n",
    "        order = rest[iou < iou_threshold]\n",
    "    return [boxes_scores[i] for i in keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "2de695d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "def suppress_nested_and_duplicates(dets, center_dist_thresh=6, nested_area_ratio=0.85):\n",
    "    \"\"\"\n",
    "    dets: list [x1,y1,x2,y2,score]\n",
    "    Removes:\n",
    "      - boxes largely contained (>nested_area_ratio of smaller area inside larger)\n",
    "      - near-duplicate boxes whose centers are very close and IoU moderate/high\n",
    "    \"\"\"\n",
    "    if not dets:\n",
    "        return dets\n",
    "    # Sort by score desc\n",
    "    dets = sorted(dets, key=lambda d: d[4], reverse=True)\n",
    "    kept = []\n",
    "    for d in dets:\n",
    "        x1,y1,x2,y2,sc = d\n",
    "        cx = 0.5*(x1+x2); cy = 0.5*(y1+y2)\n",
    "        area_d = max(1,(x2-x1)*(y2-y1))\n",
    "        discard = False\n",
    "        for k in kept:\n",
    "            kx1,ky1,kx2,ky2,ks = k\n",
    "            kc = (0.5*(kx1+kx2), 0.5*(ky1+ky2))\n",
    "            # center distance\n",
    "            if abs(cx-kc[0]) <= center_dist_thresh and abs(cy-kc[1]) <= center_dist_thresh:\n",
    "                # treat as duplicate\n",
    "                discard = True\n",
    "                break\n",
    "            # nested check\n",
    "            inter_x1 = max(x1,kx1); inter_y1 = max(y1,ky1)\n",
    "            inter_x2 = min(x2,kx2); inter_y2 = min(y2,ky2)\n",
    "            iw = max(0, inter_x2 - inter_x1)\n",
    "            ih = max(0, inter_y2 - inter_y1)\n",
    "            inter = iw*ih\n",
    "            if inter > 0:\n",
    "                area_k = max(1,(kx2-kx1)*(ky2-ky1))\n",
    "                smaller = min(area_d, area_k)\n",
    "                if inter / smaller >= nested_area_ratio:\n",
    "                    discard = True\n",
    "                    break\n",
    "        if not discard:\n",
    "            kept.append(d)\n",
    "    return kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "24d73f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_morphological_noise_removal(gray_img):\n",
    "    \"\"\"Remove noise using erosion, dilation, opening, and closing\"\"\"\n",
    "    \n",
    "    # 1. Initial thresholding\n",
    "    _, binary = cv2.threshold(gray_img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # 2. Remove small noise with opening (erosion followed by dilation)\n",
    "    noise_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2))\n",
    "    cleaned = cv2.morphologyEx(binary, cv2.MORPH_OPEN, noise_kernel)\n",
    "    \n",
    "    # 3. Remove thin lines (vertical and horizontal)\n",
    "    # Vertical line removal\n",
    "    vertical_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 8))\n",
    "    vertical_lines = cv2.morphologyEx(cleaned, cv2.MORPH_OPEN, vertical_kernel)\n",
    "    cleaned = cv2.subtract(cleaned, vertical_lines)\n",
    "    \n",
    "    # Horizontal line removal  \n",
    "    horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (8, 1))\n",
    "    horizontal_lines = cv2.morphologyEx(cleaned, cv2.MORPH_OPEN, horizontal_kernel)\n",
    "    cleaned = cv2.subtract(cleaned, horizontal_lines)\n",
    "    \n",
    "    # 4. Fill small gaps in digits with closing (dilation followed by erosion)\n",
    "    fill_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2))\n",
    "    cleaned = cv2.morphologyEx(cleaned, cv2.MORPH_CLOSE, fill_kernel)\n",
    "    \n",
    "    # 5. Remove remaining small connected components\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(cleaned)\n",
    "    min_area = 25  # Minimum area for digit components\n",
    "    final_cleaned = np.zeros_like(cleaned)\n",
    "    \n",
    "    for i in range(1, num_labels):\n",
    "        if stats[i, cv2.CC_STAT_AREA] >= min_area:\n",
    "            final_cleaned[labels == i] = 255\n",
    "    \n",
    "    # 6. Final dilation to restore digit thickness\n",
    "    restore_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (1, 1))\n",
    "    final_cleaned = cv2.dilate(final_cleaned, restore_kernel, iterations=1)\n",
    "    \n",
    "    # Convert back to grayscale (invert back)\n",
    "    result = cv2.bitwise_not(final_cleaned)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def robust_preprocessing(img_path, noise_reduction=True, contrast_enhance=True):\n",
    "    \"\"\"Enhanced preprocessing with morphological operations\"\"\"\n",
    "    \n",
    "    # Load image\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # 1. Initial denoising\n",
    "    img = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "    \n",
    "    # 2. Advanced morphological noise removal\n",
    "    img = advanced_morphological_noise_removal(img)\n",
    "    \n",
    "    # 3. Additional erosion to separate touching components\n",
    "    separate_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (1, 1))\n",
    "    img = cv2.erode(img, separate_kernel, iterations=1)\n",
    "    \n",
    "    # 4. Final contrast enhancement\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    img = clahe.apply(img)\n",
    "    \n",
    "    return img\n",
    "\n",
    "def detect_with_multiple_thresholds(gray_img, template, thresholds=[0.3, 0.4, 0.5]):\n",
    "    \"\"\"Try multiple thresholds and combine results\"\"\"\n",
    "    all_detections = []\n",
    "    \n",
    "    for thresh in thresholds:\n",
    "        res = cv2.matchTemplate(gray_img, template, cv2.TM_CCOEFF_NORMED)\n",
    "        loc = np.where(res >= thresh)\n",
    "        \n",
    "        for y, x in zip(loc[0], loc[1]):\n",
    "            score = res[y, x]\n",
    "            x1, y1 = x, y\n",
    "            x2, y2 = x + template.shape[1], y + template.shape[0]\n",
    "            all_detections.append([x1, y1, x2, y2, score])\n",
    "    \n",
    "    return all_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8cc59bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_boxes_only(img_path, boxes, out_path=\"boxes_only.png\"):\n",
    "    img = cv2.imread(img_path)\n",
    "    for (x1,y1,x2,y2) in boxes:\n",
    "        cv2.rectangle(img, (x1,y1), (x2,y2), (0,255,0), 2)\n",
    "    cv2.imwrite(out_path, img)\n",
    "    print(\"Saved\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "32722051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_feature_matching_with_edge_filtering(img_path, templates_features, similarity_thresh=0.5):\n",
    "    \"\"\"CNN matching with edge filtering to avoid corner detections\"\"\"\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_features = extract_features(img)\n",
    "    \n",
    "    detections = []\n",
    "    feat_h, feat_w = img_features.shape[1], img_features.shape[2]\n",
    "    \n",
    "    original_img = cv2.imread(img_path)\n",
    "    orig_h, orig_w = original_img.shape[:2]\n",
    "    stride_h = orig_h / feat_h\n",
    "    stride_w = orig_w / feat_w\n",
    "    \n",
    "    # Define edge margins (avoid detections too close to edges)\n",
    "    edge_margin_x = int(orig_w * 0.05)  # 5% margin from edges\n",
    "    edge_margin_y = int(orig_h * 0.05)\n",
    "    \n",
    "    for digit, template_list in templates_features.items():\n",
    "        digit_detections = 0\n",
    "        \n",
    "        for template_feat in template_list:\n",
    "            tpl_features = template_feat['features']\n",
    "            tpl_h, tpl_w = tpl_features.shape[1], tpl_features.shape[2]\n",
    "            \n",
    "            if feat_h < tpl_h or feat_w < tpl_w:\n",
    "                continue\n",
    "                \n",
    "            for y in range(0, feat_h - tpl_h + 1, 2):\n",
    "                for x in range(0, feat_w - tpl_w + 1, 2):\n",
    "                    img_patch = img_features[:, y:y+tpl_h, x:x+tpl_w]\n",
    "                    \n",
    "                    img_flat = F.normalize(img_patch.flatten(), dim=0)\n",
    "                    tpl_flat = F.normalize(tpl_features.flatten(), dim=0)\n",
    "                    \n",
    "                    similarity = F.cosine_similarity(\n",
    "                        img_flat.unsqueeze(0),\n",
    "                        tpl_flat.unsqueeze(0),\n",
    "                        dim=1\n",
    "                    ).item()\n",
    "                    \n",
    "                    if similarity > similarity_thresh:\n",
    "                        center_x = (x + tpl_w/2) * stride_w\n",
    "                        center_y = (y + tpl_h/2) * stride_h\n",
    "                        \n",
    "                        # FILTER OUT EDGE DETECTIONS\n",
    "                        if (center_x < edge_margin_x or center_x > orig_w - edge_margin_x or\n",
    "                            center_y < edge_margin_y or center_y > orig_h - edge_margin_y):\n",
    "                            continue  # Skip edge detections\n",
    "                        \n",
    "                        template_width = template_feat['width'] * 1.5  # Reduced scale\n",
    "                        template_height = template_feat['height'] * 1.5\n",
    "                        \n",
    "                        px1 = int(center_x - template_width/2)\n",
    "                        py1 = int(center_y - template_height/2)\n",
    "                        px2 = int(center_x + template_width/2)\n",
    "                        py2 = int(center_y + template_height/2)\n",
    "                        \n",
    "                        px1 = max(0, px1)\n",
    "                        py1 = max(0, py1)\n",
    "                        px2 = min(orig_w, px2)\n",
    "                        py2 = min(orig_h, py2)\n",
    "                        \n",
    "                        detections.append([px1, py1, px2, py2, similarity])\n",
    "                        digit_detections += 1\n",
    "        \n",
    "        if digit_detections > 0:\n",
    "            print(f\"Digit {digit}: {digit_detections} detections\")\n",
    "    \n",
    "    return detections\n",
    "\n",
    "def detect_digits_cnn_filtered(img_path, template_features, similarity_thresh=0.6):\n",
    "    \"\"\"CNN detection with edge filtering and higher threshold\"\"\"\n",
    "    \n",
    "    all_detections = cnn_feature_matching_with_edge_filtering(\n",
    "        img_path, \n",
    "        template_features, \n",
    "        similarity_thresh=similarity_thresh  # Higher threshold\n",
    "    )\n",
    "    \n",
    "    # More aggressive NMS\n",
    "    kept = nms_xyxy(all_detections, iou_threshold=0.3)  # Lower IoU = more aggressive\n",
    "    kept = suppress_nested_and_duplicates(kept, center_dist_thresh=15)\n",
    "    \n",
    "    boxes = [[x1, y1, x2, y2] for x1, y1, x2, y2, _ in kept]\n",
    "    boxes = sorted(boxes, key=lambda b: b[0])\n",
    "    \n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9805bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit 0: 12 detections\n",
      "Digit 2: 10 detections\n",
      "Digit 5: 2 detections\n",
      "Digit 6: 13 detections\n",
      "Digit 8: 1 detections\n",
      "Digit 9: 8 detections\n",
      "Saved result_cnn_filtered.png\n",
      "Filtered CNN detected 6 boxes\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "boxes_filtered = detect_digits_cnn_filtered(\n",
    "    r\"captcha\\21136.png\", \n",
    "    template_features, \n",
    "    similarity_thresh=0.4  # Higher threshold\n",
    ")\n",
    "visualize_boxes_only(r\"captcha\\21136.png\", boxes_filtered, \"result_cnn_filtered.png\")\n",
    "print(f\"Filtered CNN detected {len(boxes_filtered)} boxes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161ab7d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
