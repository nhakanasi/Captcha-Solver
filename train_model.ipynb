{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e23088c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import CaptchaTrainer\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82e3f997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized successfully!\n",
      "Vocabulary size: 14\n",
      "Training data size: 510\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "trainer = CaptchaTrainer()\n",
    "print(\"Trainer initialized successfully!\")\n",
    "print(f\"Vocabulary size: {len(trainer.vocab)}\")\n",
    "print(f\"Training data size: {len(trainer.train_gen)}\")\n",
    "print(f\"Device: {next(trainer.model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bff43ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "save_every = 1\n",
    "eval_every = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c345d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(\n",
    "    trainer.valid_gen,  # Assuming train_gen is the dataset or generator\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "542b2bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6648\n",
      "Validation Loss: 0.8166, Accuracy: 90.00% \n",
      "\n",
      "Checkpoint saved: checkpoints/model_epoch_1.pth \n",
      "\n",
      "Sample Accuracy: 0.00% (0/5)\n",
      "  1. Pred: '' | Actual: '96997'\n",
      "  2. Pred: '' | Actual: '66704'\n",
      "  3. Pred: '' | Actual: '50309'\n",
      "  4. Pred: '' | Actual: '76277'\n",
      "  5. Pred: '' | Actual: '63932'\n",
      "Epoch 2/10 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6615\n",
      "Validation Loss: 0.9659, Accuracy: 88.33% \n",
      "\n",
      "Checkpoint saved: checkpoints/model_epoch_2.pth \n",
      "\n",
      "Sample Accuracy: 0.00% (0/5)\n",
      "  1. Pred: '' | Actual: '96997'\n",
      "  2. Pred: '' | Actual: '66704'\n",
      "  3. Pred: '' | Actual: '50309'\n",
      "  4. Pred: '' | Actual: '76277'\n",
      "  5. Pred: '' | Actual: '63932'\n",
      "Epoch 3/10 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6559\n",
      "Validation Loss: 0.8275, Accuracy: 88.33% \n",
      "\n",
      "Checkpoint saved: checkpoints/model_epoch_3.pth \n",
      "\n",
      "Sample Accuracy: 0.00% (0/5)\n",
      "  1. Pred: '6668' | Actual: '96997'\n",
      "  2. Pred: '66668' | Actual: '66704'\n",
      "  3. Pred: '6668' | Actual: '50309'\n",
      "  4. Pred: '66666' | Actual: '76277'\n",
      "  5. Pred: '86' | Actual: '63932'\n",
      "Epoch 4/10 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6556\n",
      "Validation Loss: 0.9881, Accuracy: 85.00% \n",
      "\n",
      "Checkpoint saved: checkpoints/model_epoch_4.pth \n",
      "\n",
      "Sample Accuracy: 0.00% (0/5)\n",
      "  1. Pred: '' | Actual: '96997'\n",
      "  2. Pred: '' | Actual: '66704'\n",
      "  3. Pred: '' | Actual: '50309'\n",
      "  4. Pred: '' | Actual: '76277'\n",
      "  5. Pred: '' | Actual: '63932'\n",
      "Epoch 5/10 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6744\n",
      "Validation Loss: 0.8519, Accuracy: 88.33% \n",
      "\n",
      "Checkpoint saved: checkpoints/model_epoch_5.pth \n",
      "\n",
      "Sample Accuracy: 0.00% (0/5)\n",
      "  1. Pred: '' | Actual: '96997'\n",
      "  2. Pred: '' | Actual: '66704'\n",
      "  3. Pred: '' | Actual: '50309'\n",
      "  4. Pred: '' | Actual: '76277'\n",
      "  5. Pred: '' | Actual: '63932'\n",
      "Epoch 6/10 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6584\n",
      "Validation Loss: 0.7759, Accuracy: 91.67% \n",
      "\n",
      "Checkpoint saved: checkpoints/model_epoch_6.pth \n",
      "\n",
      "Sample Accuracy: 0.00% (0/5)\n",
      "  1. Pred: '' | Actual: '96997'\n",
      "  2. Pred: '' | Actual: '66704'\n",
      "  3. Pred: '' | Actual: '50309'\n",
      "  4. Pred: '' | Actual: '76277'\n",
      "  5. Pred: '' | Actual: '63932'\n",
      "Epoch 7/10 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6517\n",
      "Validation Loss: 0.7874, Accuracy: 91.67% \n",
      "\n",
      "Checkpoint saved: checkpoints/model_epoch_7.pth \n",
      "\n",
      "Sample Accuracy: 0.00% (0/5)\n",
      "  1. Pred: '' | Actual: '96997'\n",
      "  2. Pred: '' | Actual: '66704'\n",
      "  3. Pred: '' | Actual: '50309'\n",
      "  4. Pred: '' | Actual: '76277'\n",
      "  5. Pred: '' | Actual: '63932'\n",
      "Epoch 8/10 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6391\n",
      "Validation Loss: 0.8174, Accuracy: 88.33% \n",
      "\n",
      "Checkpoint saved: checkpoints/model_epoch_8.pth \n",
      "\n",
      "Sample Accuracy: 0.00% (0/5)\n",
      "  1. Pred: '' | Actual: '96997'\n",
      "  2. Pred: '' | Actual: '66704'\n",
      "  3. Pred: '' | Actual: '50309'\n",
      "  4. Pred: '' | Actual: '76277'\n",
      "  5. Pred: '' | Actual: '63932'\n",
      "Epoch 9/10 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6431\n",
      "Validation Loss: 0.7208, Accuracy: 95.00% \n",
      "\n",
      "Checkpoint saved: checkpoints/model_epoch_9.pth \n",
      "\n",
      "Sample Accuracy: 0.00% (0/5)\n",
      "  1. Pred: '' | Actual: '96997'\n",
      "  2. Pred: '' | Actual: '66704'\n",
      "  3. Pred: '' | Actual: '50309'\n",
      "  4. Pred: '' | Actual: '76277'\n",
      "  5. Pred: '' | Actual: '63932'\n",
      "Epoch 10/10 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6504\n",
      "Validation Loss: 0.9120, Accuracy: 88.33% \n",
      "\n",
      "Checkpoint saved: checkpoints/model_epoch_10.pth \n",
      "\n",
      "Sample Accuracy: 0.00% (0/5)\n",
      "  1. Pred: '' | Actual: '96997'\n",
      "  2. Pred: '' | Actual: '66704'\n",
      "  3. Pred: '' | Actual: '50309'\n",
      "  4. Pred: '' | Actual: '76277'\n",
      "  5. Pred: '' | Actual: '63932'\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "eval_losses = []\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    print(f\"Epoch {epoch}/{num_epochs} \\n\")\n",
    "    \n",
    "    # Training\n",
    "    trainer.train_epoch(epoch)\n",
    "    epoch_loss = trainer.train_loss[-1]\n",
    "    train_losses.append(epoch_loss)\n",
    "    \n",
    "    print(f\"Training Loss: {epoch_loss:.4f}\")\n",
    "    if epoch % eval_every == 0:\n",
    "        try:\n",
    "            eval_loss, eval_accuracy = trainer.evaluate(val_loader)\n",
    "            eval_losses.append(eval_loss)\n",
    "            print(f\"Validation Loss: {eval_loss:.4f}, Accuracy: {eval_accuracy:.2f}% \\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Evaluation failed: {e} \\n\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if epoch % save_every == 0:\n",
    "        checkpoint_path = f\"checkpoints/model_epoch_{epoch}.pth\"\n",
    "        trainer.save_checkpoint(checkpoint_path)\n",
    "        print(f\"Checkpoint saved: {checkpoint_path} \\n\")\n",
    "    \n",
    "    # Prediction sample\n",
    "    if epoch % eval_every == 0:\n",
    "        try:\n",
    "            pred_sents, actual_sents = trainer.predict(data_loader=val_loader, sample=5, use_beam_search= True)\n",
    "            correct = sum(1 for pred, actual in zip(pred_sents, actual_sents) \n",
    "                        if pred.strip() == actual.strip())\n",
    "            accuracy = correct / len(pred_sents) * 100\n",
    "            print(f\"Sample Accuracy: {accuracy:.2f}% ({correct}/{len(pred_sents)})\")\n",
    "            for i, (pred, actual) in enumerate(zip(pred_sents[:], actual_sents[:])):\n",
    "                print(f\"  {i+1}. Pred: '{pred}' | Actual: '{actual}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"Prediction failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "726185d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ALIENWARE\\OneDrive - Hanoi University of Science and Technology\\HUST\\BKAI\\Self-taught\\Captcha\\model\\transformer.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pe = torch.tensor(self.pe[:,:seq_length, :self.d_model],requires_grad=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, next_tokens: tensor([13, 10,  9, 11, 10,  6,  4,  9])\n",
      "Step 1, next_tokens: tensor([13, 10,  4, 10,  7,  9, 11,  9])\n",
      "Step 2, next_tokens: tensor([11, 11,  7,  6, 13, 13, 11, 13])\n",
      "Step 3, next_tokens: tensor([13,  4,  4, 11,  7,  6, 11, 13])\n",
      "Step 4, next_tokens: tensor([11,  8, 13,  2,  6,  7,  6, 10])\n",
      "Step 0, next_tokens: tensor([9, 6])\n",
      "Step 1, next_tokens: tensor([ 4, 13])\n",
      "Step 2, next_tokens: tensor([13, 13])\n",
      "Step 3, next_tokens: tensor([ 7, 10])\n",
      "Step 4, next_tokens: tensor([ 4, 11])\n",
      "Sample Accuracy: 80.00% (8/10)\n",
      "  1. Pred: '99797' | Actual: '96997'\n",
      "  2. Pred: '66704' | Actual: '66704'\n",
      "  3. Pred: '50309' | Actual: '50309'\n",
      "  4. Pred: '7627' | Actual: '76277'\n",
      "  5. Pred: '63932' | Actual: '63932'\n",
      "  6. Pred: '25923' | Actual: '25923'\n",
      "  7. Pred: '07772' | Actual: '07772'\n",
      "  8. Pred: '55996' | Actual: '55996'\n",
      "  9. Pred: '50930' | Actual: '50930'\n",
      "  10. Pred: '29967' | Actual: '29967'\n"
     ]
    }
   ],
   "source": [
    "trainer.load_checkpoint('./checkpoints/model_epoch_9.pth')\n",
    "pred_sents, actual_sents = trainer.predict(data_loader=val_loader, sample=10, use_beam_search= False)\n",
    "correct = sum(1 for pred, actual in zip(pred_sents, actual_sents) \n",
    "            if pred.strip() == actual.strip())\n",
    "accuracy = correct / len(pred_sents) * 100\n",
    "print(f\"Sample Accuracy: {accuracy:.2f}% ({correct}/{len(pred_sents)})\")\n",
    "for i, (pred, actual) in enumerate(zip(pred_sents[:], actual_sents[:])):\n",
    "    print(f\"  {i+1}. Pred: '{pred}' | Actual: '{actual}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9d883d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils.vocab import Vocab\n",
    "from PIL import Image\n",
    "import torch\n",
    "import albumentations as A\n",
    "def process_image(img):\n",
    "    img = img.resize((200, 80), Image.LANCZOS)  # Fixed size\n",
    "    img = np.asarray(img).transpose(2, 0, 1)\n",
    "    img = img / 255\n",
    "    return img\n",
    "class PrivateDataset(Dataset):\n",
    "    def __init__(self, annote_file, image_dir):\n",
    "        self.image_dir = image_dir\n",
    "        \n",
    "        # Read annotation file\n",
    "        with open(annote_file, 'r') as f:\n",
    "            self.image_files = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        \n",
    "        # Load image and apply same preprocessing as training\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Use the same process_image function as training (no augmentation)\n",
    "        processed_img = process_image(image)  # This resizes to (200, 80) and normalizes\n",
    "        \n",
    "        # Return filename as identifier (no label available)\n",
    "        filename = self.image_files[idx]\n",
    "        \n",
    "        return torch.FloatTensor(processed_img), filename\n",
    "\n",
    "# Create private dataset and dataloader\n",
    "def create_private_dataloader(annote_file, image_dir, batch_size=8):\n",
    "    dataset = PrivateDataset(annote_file, image_dir)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "# Prediction function for private set (no labels)\n",
    "def predict_private_set(trainer, annote_file, image_dir, use_beam_search=True):\n",
    "    \"\"\"\n",
    "    Generate predictions for private test set (no ground truth labels)\n",
    "    \"\"\"\n",
    "    print(\"Creating private dataset...\")\n",
    "    private_loader = create_private_dataloader(annote_file, image_dir)\n",
    "    \n",
    "    print(f\"Private dataset size: {len(private_loader.dataset)}\")\n",
    "    \n",
    "    # Get predictions only (no accuracy calculation)\n",
    "    print(\"Running predictions...\")\n",
    "    predictions = []\n",
    "    filenames = []\n",
    "    \n",
    "    trainer.model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_img, batch_filenames in private_loader:\n",
    "            batch_img = batch_img.to(trainer.device)\n",
    "            batch_size = batch_img.size(0)\n",
    "            \n",
    "            # Use trainer.predict method for both beam search and autoregressive\n",
    "            batch_predictions = trainer.predict(img=batch_img, use_beam_search=use_beam_search)\n",
    "            \n",
    "            # Add predictions and filenames\n",
    "            for b in range(batch_size):\n",
    "                predictions.append(batch_predictions[b])\n",
    "                filenames.append(batch_filenames[b])\n",
    "    \n",
    "    print(f\"\\n=== PRIVATE SET PREDICTIONS ===\")\n",
    "    print(f\"Total samples: {len(predictions)}\")\n",
    "    \n",
    "    # Show sample predictions\n",
    "    print(f\"\\nSample predictions:\")\n",
    "    for i, (filename, pred) in enumerate(zip(filenames[:10], predictions[:10])):\n",
    "        print(f\"  {i+1:2d}. {filename} -> '{pred}'\")\n",
    "    \n",
    "    # Save results to submission file\n",
    "    results_file = \"private_predictions.txt\"\n",
    "    with open(results_file, 'w') as f:\n",
    "        for filename, pred in zip(filenames, predictions):\n",
    "            f.write(f\"{filename}\\t{pred}\\n\")\n",
    "    \n",
    "    print(f\"\\nPredictions saved to: {results_file}\")\n",
    "    \n",
    "    return predictions, filenames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0310d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating private dataset...\n",
      "Private dataset size: 20\n",
      "Running predictions...\n",
      "\n",
      "=== PRIVATE SET PREDICTIONS ===\n",
      "Total samples: 20\n",
      "\n",
      "Sample predictions:\n",
      "   1. private/captcha_0000.jpg -> '290'\n",
      "   2. private/captcha_0001.jpg -> '37774'\n",
      "   3. private/captcha_0002.jpg -> '65236'\n",
      "   4. private/captcha_0003.jpg -> '39468'\n",
      "   5. private/captcha_0004.jpg -> '23039'\n",
      "   6. private/captcha_0005.jpg -> '034'\n",
      "   7. private/captcha_0006.jpg -> '63890'\n",
      "   8. private/captcha_0007.jpg -> '02788'\n",
      "   9. private/captcha_0008.jpg -> '64040'\n",
      "  10. private/captcha_0009.jpg -> '8766'\n",
      "\n",
      "Predictions saved to: private_predictions.txt\n"
     ]
    }
   ],
   "source": [
    "from data.dataloader import *\n",
    "predictions, filenames = predict_private_set(trainer, 'private_annote.txt', '', use_beam_search=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e22fff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
